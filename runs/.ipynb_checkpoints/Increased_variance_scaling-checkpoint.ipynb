{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import os \n",
    "import h5py\n",
    "import sys\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.listdir('./../../../../data/hongtao'))\n",
    "\n",
    "# path = os.path('./../../../../data/hongtao/variables_tt_re1.h5')\n",
    "\n",
    "# os.path.abspath(\"data/hongtao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['j1_eta', 'j1_isbtag', 'j1_m', 'j1_phi', 'j1_pt', 'j2_eta', 'j2_isbtag', 'j2_m', 'j2_phi', 'j2_pt', 'j3_eta', 'j3_isbtag', 'j3_m', 'j3_phi', 'j3_pt', 'j4_eta', 'j4_isbtag', 'j4_m', 'j4_phi', 'j4_pt', 'j5_eta', 'j5_isbtag', 'j5_m', 'j5_phi', 'j5_pt', 'j6_eta', 'j6_isbtag', 'j6_m', 'j6_phi', 'j6_pt', 'j7_eta', 'j7_isbtag', 'j7_m', 'j7_phi', 'j7_pt', 'j8_eta', 'j8_isbtag', 'j8_m', 'j8_phi', 'j8_pt', 'lep_eta', 'lep_m', 'lep_phi', 'lep_pt', 'met_met', 'met_phi', 'mttReco', 'mttTrue']>\n",
      "\n",
      "<KeysViewHDF5 ['bh_eta', 'bh_phi', 'bh_pt', 'bl_eta', 'bl_phi', 'bl_pt', 'j1_DL1r', 'j1_eta', 'j1_isbtag', 'j1_m', 'j1_phi', 'j1_pt', 'j2_DL1r', 'j2_eta', 'j2_isbtag', 'j2_m', 'j2_phi', 'j2_pt', 'j3_DL1r', 'j3_eta', 'j3_isbtag', 'j3_m', 'j3_phi', 'j3_pt', 'j4_DL1r', 'j4_eta', 'j4_isbtag', 'j4_m', 'j4_phi', 'j4_pt', 'j5_DL1r', 'j5_eta', 'j5_isbtag', 'j5_m', 'j5_phi', 'j5_pt', 'j6_DL1r', 'j6_eta', 'j6_isbtag', 'j6_m', 'j6_phi', 'j6_pt', 'j7_DL1r', 'j7_eta', 'j7_isbtag', 'j7_m', 'j7_phi', 'j7_pt', 'j8_DL1r', 'j8_eta', 'j8_isbtag', 'j8_m', 'j8_phi', 'j8_pt', 'lep_eta', 'lep_m', 'lep_phi', 'lep_pt', 'met_met', 'met_phi', 'mttReco', 'mttTrue', 'th_eta', 'th_phi', 'th_pt', 'tl_eta', 'tl_phi', 'tl_pt', 'wh_eta', 'wh_phi', 'wh_pt', 'wl_eta', 'wl_phi', 'wl_pt']>\n"
     ]
    }
   ],
   "source": [
    "# os.chdir('./../../../../../data/hongtao')\n",
    "be = h5py.File('./../../../../../data/hongtao/variables_tt_re.h5','r')\n",
    "bmu = h5py.File('./../../../../../data/hongtao/variables_tt_rmu.h5','r')\n",
    "be1 = h5py.File('./../../../../../data/hongtao/variables_tt_re1.h5','r')\n",
    "print(be.keys())\n",
    "print('')\n",
    "print(be1.keys())\n",
    "dataset = be1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_keys = ['j1_pt', 'j1_eta', 'j1_phi', 'j1_m', 'j1_DL1r', 'j2_pt', 'j2_eta', 'j2_phi', 'j2_m', 'j2_DL1r', 'j3_pt', 'j3_eta', 'j3_phi', 'j3_m', 'j3_DL1r', 'j4_pt', 'j4_eta', 'j4_phi', 'j4_m', 'j4_DL1r', 'j5_pt', 'j5_eta', 'j5_phi', 'j5_m', 'j5_DL1r', 'j6_pt', 'j6_eta', 'j6_phi', 'j6_m', 'j6_DL1r', 'j7_pt', 'j7_eta', 'j7_phi','j7_m', 'j7_DL1r', 'j8_pt', 'j8_eta', 'j8_phi','j8_m', 'j8_DL1r', 'lep_pt', 'lep_eta', 'lep_phi', 'lep_m', 'met_met', 'met_phi'] \n",
    "output_keys = ['th_pt', 'th_eta','th_phi', 'tl_pt', 'tl_eta', 'tl_phi']\n",
    "# output_keys = ['th_eta','th_phi', 'tl_eta', 'tl_phi']\n",
    "\n",
    "output_length = len(output_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into numpy arrays and mean normalize\n",
    "crop =  100000\n",
    "\n",
    "input_arrays = []\n",
    "output_arrays = [] \n",
    "in_std = []\n",
    "in_mean = []\n",
    "out_std = []\n",
    "out_mean = []\n",
    "\n",
    "# for key in input_keys:\n",
    "#     exec(key + ' = np.array(dataset.get(key))[0:crop].reshape((1,-1))')\n",
    "#     exec('{0} = ({0} - np.mean({0}))/np.std({0})'.format(key))\n",
    "#     exec('input_arrays.append({0})'.format(key))\n",
    "\n",
    "    \n",
    "def convert_normalize(keys, std, mean, total):\n",
    "    for x in keys:\n",
    "        var = np.array(dataset.get(x))[0:crop].reshape((1,-1))\n",
    "        sig = np.std(var)\n",
    "        mu = np.mean(var)\n",
    "        std.append(np.std(var))\n",
    "        mean.append(np.mean(var))\n",
    "        var = (var - mu)/sig \n",
    "        total.append(var)\n",
    "\n",
    "    \n",
    "convert_normalize(input_keys, in_std, in_mean, input_arrays)\n",
    "convert_normalize(output_keys, out_std, out_mean, output_arrays)\n",
    "\n",
    "out_std = np.array(out_std).reshape((1,-1))\n",
    "out_mean = np.array(out_mean).reshape((1,-1))\n",
    "in_std = np.array(in_std).reshape((1,-1))\n",
    "in_mean = np.array(in_mean).reshape((1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input and test array\n",
    "split = int(np.floor(0.8*crop)) # 80/20 split \n",
    "shuffle = True\n",
    "    \n",
    "total_input = np.stack(input_arrays).reshape((crop, len(input_keys)))\n",
    "total_output = np.stack(output_arrays).reshape((crop, len(output_keys)))\n",
    "\n",
    "if shuffle:\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(total_input)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(total_output)\n",
    "    \n",
    "# size = total_input.shape\n",
    "# total_input = np.array(np.random.normal(size=total_input.shape))\n",
    "\n",
    "train_input, test_input = total_input[0:split, :], total_input[split:, :]\n",
    "train_output, test_output = total_output[0:split,:], total_output[split:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 46) (80000, 6)\n",
      "(20000, 46) (20000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape, train_output.shape)\n",
    "print(test_input.shape, test_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "\n",
    "# optimizer = keras.optimizers.RMSprop(learning_rate = 1e-5)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.0001),\n",
    "                     input_shape=[len(input_keys)]),\n",
    "        layers.Dense(128, kernel_regularizer=regularizers.l2(0.0001), activation='relu'),\n",
    "        layers.Dense(64, kernel_regularizer=regularizers.l2(0.0001), activation='relu'),\n",
    "        layers.Dense(len(output_keys), kernel_regularizer=regularizers.l2(0.0001), activation='linear')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='mse', optimizer= optimizer, metrics=['mse'])\n",
    "    return model \n",
    "\n",
    "# def build_model():\n",
    "#     model = keras.Sequential([\n",
    "#         layers.Dense(6, activation='linear', \n",
    "#                      input_shape=[len(input_keys)])])\n",
    "    \n",
    "#     model.compile(loss='mse', optimizer= optimizer, metrics=['mse'])\n",
    "    \n",
    "#     return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               12032     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 53,574\n",
      "Trainable params: 53,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/512\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 1.0745 - mse: 1.0401 - val_loss: 1.0446 - val_mse: 1.0104\n",
      "Epoch 2/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0486 - mse: 1.0146 - val_loss: 1.0374 - val_mse: 1.0035\n",
      "Epoch 3/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0426 - mse: 1.0089 - val_loss: 1.0343 - val_mse: 1.0007\n",
      "Epoch 4/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0393 - mse: 1.0059 - val_loss: 1.0327 - val_mse: 0.9995\n",
      "Epoch 5/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0369 - mse: 1.0039 - val_loss: 1.0316 - val_mse: 0.9987\n",
      "Epoch 6/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0350 - mse: 1.0023 - val_loss: 1.0308 - val_mse: 0.9983\n",
      "Epoch 7/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0333 - mse: 1.0009 - val_loss: 1.0302 - val_mse: 0.9981\n",
      "Epoch 8/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0318 - mse: 0.9998 - val_loss: 1.0298 - val_mse: 0.9979\n",
      "Epoch 9/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0303 - mse: 0.9986 - val_loss: 1.0295 - val_mse: 0.9979\n",
      "Epoch 10/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0290 - mse: 0.9976 - val_loss: 1.0291 - val_mse: 0.9978\n",
      "Epoch 11/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0277 - mse: 0.9966 - val_loss: 1.0287 - val_mse: 0.9978\n",
      "Epoch 12/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0265 - mse: 0.9957 - val_loss: 1.0285 - val_mse: 0.9978\n",
      "Epoch 13/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0254 - mse: 0.9948 - val_loss: 1.0282 - val_mse: 0.9978\n",
      "Epoch 14/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0242 - mse: 0.9940 - val_loss: 1.0281 - val_mse: 0.9979\n",
      "Epoch 15/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0232 - mse: 0.9931 - val_loss: 1.0279 - val_mse: 0.9980\n",
      "Epoch 16/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0221 - mse: 0.9923 - val_loss: 1.0279 - val_mse: 0.9982\n",
      "Epoch 17/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0211 - mse: 0.9915 - val_loss: 1.0277 - val_mse: 0.9983\n",
      "Epoch 18/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0201 - mse: 0.9907 - val_loss: 1.0277 - val_mse: 0.9984\n",
      "Epoch 19/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0192 - mse: 0.9900 - val_loss: 1.0278 - val_mse: 0.9987\n",
      "Epoch 20/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0182 - mse: 0.9892 - val_loss: 1.0276 - val_mse: 0.9987\n",
      "Epoch 21/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0173 - mse: 0.9884 - val_loss: 1.0278 - val_mse: 0.9990\n",
      "Epoch 22/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0163 - mse: 0.9876 - val_loss: 1.0279 - val_mse: 0.9992\n",
      "Epoch 23/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0154 - mse: 0.9868 - val_loss: 1.0279 - val_mse: 0.9994\n",
      "Epoch 24/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0145 - mse: 0.9861 - val_loss: 1.0281 - val_mse: 0.9997\n",
      "Epoch 25/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0136 - mse: 0.9853 - val_loss: 1.0282 - val_mse: 0.9999\n",
      "Epoch 26/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0127 - mse: 0.9845 - val_loss: 1.0284 - val_mse: 1.0002\n",
      "Epoch 27/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0119 - mse: 0.9837 - val_loss: 1.0285 - val_mse: 1.0004\n",
      "Epoch 28/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0110 - mse: 0.9829 - val_loss: 1.0289 - val_mse: 1.0008\n",
      "Epoch 29/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0101 - mse: 0.9821 - val_loss: 1.0292 - val_mse: 1.0012\n",
      "Epoch 30/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0092 - mse: 0.9813 - val_loss: 1.0293 - val_mse: 1.0014\n",
      "Epoch 31/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0084 - mse: 0.9805 - val_loss: 1.0298 - val_mse: 1.0019\n",
      "Epoch 32/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0075 - mse: 0.9796 - val_loss: 1.0300 - val_mse: 1.0022\n",
      "Epoch 33/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0066 - mse: 0.9788 - val_loss: 1.0304 - val_mse: 1.0026\n",
      "Epoch 34/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0058 - mse: 0.9780 - val_loss: 1.0306 - val_mse: 1.0028\n",
      "Epoch 35/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0049 - mse: 0.9771 - val_loss: 1.0309 - val_mse: 1.0031\n",
      "Epoch 36/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0040 - mse: 0.9763 - val_loss: 1.0314 - val_mse: 1.0036\n",
      "Epoch 37/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0032 - mse: 0.9754 - val_loss: 1.0316 - val_mse: 1.0038\n",
      "Epoch 38/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0024 - mse: 0.9747 - val_loss: 1.0320 - val_mse: 1.0042\n",
      "Epoch 39/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0016 - mse: 0.9738 - val_loss: 1.0325 - val_mse: 1.0047\n",
      "Epoch 40/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1.0007 - mse: 0.9729 - val_loss: 1.0333 - val_mse: 1.0055\n",
      "Epoch 41/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9999 - mse: 0.9721 - val_loss: 1.0335 - val_mse: 1.0057\n",
      "Epoch 42/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9991 - mse: 0.9713 - val_loss: 1.0337 - val_mse: 1.0058\n",
      "Epoch 43/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9983 - mse: 0.9704 - val_loss: 1.0344 - val_mse: 1.0065\n",
      "Epoch 44/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9974 - mse: 0.9695 - val_loss: 1.0348 - val_mse: 1.0069\n",
      "Epoch 45/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9965 - mse: 0.9686 - val_loss: 1.0354 - val_mse: 1.0074\n",
      "Epoch 46/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9958 - mse: 0.9678 - val_loss: 1.0357 - val_mse: 1.0077\n",
      "Epoch 47/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9950 - mse: 0.9669 - val_loss: 1.0365 - val_mse: 1.0084\n",
      "Epoch 48/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9941 - mse: 0.9660 - val_loss: 1.0371 - val_mse: 1.0090\n",
      "Epoch 49/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9933 - mse: 0.9652 - val_loss: 1.0373 - val_mse: 1.0092\n",
      "Epoch 50/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9925 - mse: 0.9643 - val_loss: 1.0381 - val_mse: 1.0098\n",
      "Epoch 51/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9917 - mse: 0.9634 - val_loss: 1.0384 - val_mse: 1.0101\n",
      "Epoch 52/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9909 - mse: 0.9626 - val_loss: 1.0393 - val_mse: 1.0109\n",
      "Epoch 53/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9900 - mse: 0.9616 - val_loss: 1.0397 - val_mse: 1.0113\n",
      "Epoch 54/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9893 - mse: 0.9608 - val_loss: 1.0405 - val_mse: 1.0120\n",
      "Epoch 55/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9885 - mse: 0.9600 - val_loss: 1.0408 - val_mse: 1.0123\n",
      "Epoch 56/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9876 - mse: 0.9590 - val_loss: 1.0418 - val_mse: 1.0132\n",
      "Epoch 57/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9869 - mse: 0.9583 - val_loss: 1.0423 - val_mse: 1.0136\n",
      "Epoch 58/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9862 - mse: 0.9574 - val_loss: 1.0433 - val_mse: 1.0145\n",
      "Epoch 59/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9854 - mse: 0.9565 - val_loss: 1.0436 - val_mse: 1.0147\n",
      "Epoch 60/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9846 - mse: 0.9557 - val_loss: 1.0439 - val_mse: 1.0149\n",
      "Epoch 61/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9838 - mse: 0.9548 - val_loss: 1.0448 - val_mse: 1.0157\n",
      "Epoch 62/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9831 - mse: 0.9540 - val_loss: 1.0453 - val_mse: 1.0162\n",
      "Epoch 63/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9823 - mse: 0.9531 - val_loss: 1.0460 - val_mse: 1.0167\n",
      "Epoch 64/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9816 - mse: 0.9523 - val_loss: 1.0471 - val_mse: 1.0177\n",
      "Epoch 65/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9809 - mse: 0.9515 - val_loss: 1.0472 - val_mse: 1.0178\n",
      "Epoch 66/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9801 - mse: 0.9507 - val_loss: 1.0481 - val_mse: 1.0186\n",
      "Epoch 67/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9794 - mse: 0.9498 - val_loss: 1.0488 - val_mse: 1.0192\n",
      "Epoch 68/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9786 - mse: 0.9490 - val_loss: 1.0493 - val_mse: 1.0196\n",
      "Epoch 69/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9779 - mse: 0.9482 - val_loss: 1.0498 - val_mse: 1.0201\n",
      "Epoch 70/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9771 - mse: 0.9473 - val_loss: 1.0507 - val_mse: 1.0208\n",
      "Epoch 71/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9764 - mse: 0.9465 - val_loss: 1.0515 - val_mse: 1.0216\n",
      "Epoch 72/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9757 - mse: 0.9458 - val_loss: 1.0518 - val_mse: 1.0217\n",
      "Epoch 73/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9750 - mse: 0.9449 - val_loss: 1.0525 - val_mse: 1.0224\n",
      "Epoch 74/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9744 - mse: 0.9442 - val_loss: 1.0538 - val_mse: 1.0236\n",
      "Epoch 75/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9736 - mse: 0.9434 - val_loss: 1.0545 - val_mse: 1.0241\n",
      "Epoch 76/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9729 - mse: 0.9426 - val_loss: 1.0549 - val_mse: 1.0245\n",
      "Epoch 77/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9722 - mse: 0.9418 - val_loss: 1.0556 - val_mse: 1.0251\n",
      "Epoch 78/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9716 - mse: 0.9410 - val_loss: 1.0564 - val_mse: 1.0258\n",
      "Epoch 79/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9709 - mse: 0.9403 - val_loss: 1.0568 - val_mse: 1.0261\n",
      "Epoch 80/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9702 - mse: 0.9395 - val_loss: 1.0578 - val_mse: 1.0270\n",
      "Epoch 81/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9696 - mse: 0.9388 - val_loss: 1.0584 - val_mse: 1.0275\n",
      "Epoch 82/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9690 - mse: 0.9381 - val_loss: 1.0586 - val_mse: 1.0276\n",
      "Epoch 83/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9683 - mse: 0.9373 - val_loss: 1.0601 - val_mse: 1.0290\n",
      "Epoch 84/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9676 - mse: 0.9364 - val_loss: 1.0609 - val_mse: 1.0297\n",
      "Epoch 85/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9669 - mse: 0.9357 - val_loss: 1.0612 - val_mse: 1.0299\n",
      "Epoch 86/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9663 - mse: 0.9350 - val_loss: 1.0621 - val_mse: 1.0308\n",
      "Epoch 87/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9656 - mse: 0.9342 - val_loss: 1.0628 - val_mse: 1.0314\n",
      "Epoch 88/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9650 - mse: 0.9334 - val_loss: 1.0632 - val_mse: 1.0317\n",
      "Epoch 89/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9644 - mse: 0.9327 - val_loss: 1.0640 - val_mse: 1.0323\n",
      "Epoch 90/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9637 - mse: 0.9319 - val_loss: 1.0650 - val_mse: 1.0332\n",
      "Epoch 91/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9632 - mse: 0.9314 - val_loss: 1.0657 - val_mse: 1.0338\n",
      "Epoch 92/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9625 - mse: 0.9306 - val_loss: 1.0664 - val_mse: 1.0344\n",
      "Epoch 93/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9618 - mse: 0.9298 - val_loss: 1.0663 - val_mse: 1.0343\n",
      "Epoch 94/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9612 - mse: 0.9291 - val_loss: 1.0674 - val_mse: 1.0352\n",
      "Epoch 95/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9606 - mse: 0.9284 - val_loss: 1.0687 - val_mse: 1.0364\n",
      "Epoch 96/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9600 - mse: 0.9277 - val_loss: 1.0692 - val_mse: 1.0368\n",
      "Epoch 97/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9595 - mse: 0.9271 - val_loss: 1.0693 - val_mse: 1.0368\n",
      "Epoch 98/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9588 - mse: 0.9263 - val_loss: 1.0704 - val_mse: 1.0379\n",
      "Epoch 99/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9582 - mse: 0.9256 - val_loss: 1.0708 - val_mse: 1.0382\n",
      "Epoch 100/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9576 - mse: 0.9250 - val_loss: 1.0710 - val_mse: 1.0382\n",
      "Epoch 101/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9570 - mse: 0.9242 - val_loss: 1.0720 - val_mse: 1.0392\n",
      "Epoch 102/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9566 - mse: 0.9237 - val_loss: 1.0730 - val_mse: 1.0401\n",
      "Epoch 103/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9559 - mse: 0.9229 - val_loss: 1.0738 - val_mse: 1.0408\n",
      "Epoch 104/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9552 - mse: 0.9222 - val_loss: 1.0740 - val_mse: 1.0409\n",
      "Epoch 105/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9549 - mse: 0.9217 - val_loss: 1.0751 - val_mse: 1.0419\n",
      "Epoch 106/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9541 - mse: 0.9208 - val_loss: 1.0757 - val_mse: 1.0424\n",
      "Epoch 107/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9535 - mse: 0.9202 - val_loss: 1.0765 - val_mse: 1.0431\n",
      "Epoch 108/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9530 - mse: 0.9196 - val_loss: 1.0769 - val_mse: 1.0434\n",
      "Epoch 109/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9525 - mse: 0.9189 - val_loss: 1.0779 - val_mse: 1.0443\n",
      "Epoch 110/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9518 - mse: 0.9182 - val_loss: 1.0782 - val_mse: 1.0445\n",
      "Epoch 111/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9513 - mse: 0.9176 - val_loss: 1.0787 - val_mse: 1.0450\n",
      "Epoch 112/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9508 - mse: 0.9170 - val_loss: 1.0793 - val_mse: 1.0455\n",
      "Epoch 113/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9502 - mse: 0.9163 - val_loss: 1.0801 - val_mse: 1.0461\n",
      "Epoch 114/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9497 - mse: 0.9157 - val_loss: 1.0805 - val_mse: 1.0465\n",
      "Epoch 115/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9491 - mse: 0.9150 - val_loss: 1.0816 - val_mse: 1.0475\n",
      "Epoch 116/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9485 - mse: 0.9144 - val_loss: 1.0819 - val_mse: 1.0477\n",
      "Epoch 117/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9480 - mse: 0.9137 - val_loss: 1.0829 - val_mse: 1.0486\n",
      "Epoch 118/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9474 - mse: 0.9130 - val_loss: 1.0829 - val_mse: 1.0485\n",
      "Epoch 119/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9470 - mse: 0.9125 - val_loss: 1.0841 - val_mse: 1.0496\n",
      "Epoch 120/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9464 - mse: 0.9118 - val_loss: 1.0845 - val_mse: 1.0499\n",
      "Epoch 121/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9460 - mse: 0.9113 - val_loss: 1.0848 - val_mse: 1.0501\n",
      "Epoch 122/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9454 - mse: 0.9107 - val_loss: 1.0860 - val_mse: 1.0512\n",
      "Epoch 123/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9449 - mse: 0.9101 - val_loss: 1.0862 - val_mse: 1.0514\n",
      "Epoch 124/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9444 - mse: 0.9095 - val_loss: 1.0874 - val_mse: 1.0524\n",
      "Epoch 125/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9438 - mse: 0.9088 - val_loss: 1.0878 - val_mse: 1.0527\n",
      "Epoch 126/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9434 - mse: 0.9083 - val_loss: 1.0888 - val_mse: 1.0537\n",
      "Epoch 127/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9428 - mse: 0.9077 - val_loss: 1.0893 - val_mse: 1.0541\n",
      "Epoch 128/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9424 - mse: 0.9071 - val_loss: 1.0898 - val_mse: 1.0545\n",
      "Epoch 129/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9419 - mse: 0.9065 - val_loss: 1.0909 - val_mse: 1.0555\n",
      "Epoch 130/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9413 - mse: 0.9059 - val_loss: 1.0909 - val_mse: 1.0554\n",
      "Epoch 131/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9409 - mse: 0.9054 - val_loss: 1.0918 - val_mse: 1.0562\n",
      "Epoch 132/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9403 - mse: 0.9046 - val_loss: 1.0916 - val_mse: 1.0560\n",
      "Epoch 133/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9398 - mse: 0.9041 - val_loss: 1.0924 - val_mse: 1.0566\n",
      "Epoch 134/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9393 - mse: 0.9035 - val_loss: 1.0935 - val_mse: 1.0576\n",
      "Epoch 135/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9389 - mse: 0.9030 - val_loss: 1.0935 - val_mse: 1.0576\n",
      "Epoch 136/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9384 - mse: 0.9024 - val_loss: 1.0943 - val_mse: 1.0583\n",
      "Epoch 137/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9380 - mse: 0.9020 - val_loss: 1.0959 - val_mse: 1.0598\n",
      "Epoch 138/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9375 - mse: 0.9014 - val_loss: 1.0957 - val_mse: 1.0595\n",
      "Epoch 139/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9370 - mse: 0.9008 - val_loss: 1.0962 - val_mse: 1.0599\n",
      "Epoch 140/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9365 - mse: 0.9002 - val_loss: 1.0968 - val_mse: 1.0604\n",
      "Epoch 141/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9360 - mse: 0.8996 - val_loss: 1.0979 - val_mse: 1.0615\n",
      "Epoch 142/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9356 - mse: 0.8991 - val_loss: 1.0981 - val_mse: 1.0616\n",
      "Epoch 143/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9351 - mse: 0.8986 - val_loss: 1.0987 - val_mse: 1.0621\n",
      "Epoch 144/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9348 - mse: 0.8982 - val_loss: 1.0995 - val_mse: 1.0628\n",
      "Epoch 145/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9342 - mse: 0.8974 - val_loss: 1.1003 - val_mse: 1.0635\n",
      "Epoch 146/512\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9337 - mse: 0.8969 - val_loss: 1.1010 - val_mse: 1.0641\n",
      "Epoch 147/512\n",
      "106/125 [========================>.....] - ETA: 0s - loss: 0.9328 - mse: 0.8959"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "Epochs= 512\n",
    "\n",
    "history = model.fit(train_input, train_output, verbose=1, epochs=Epochs, \n",
    "                   validation_split=0.2, shuffle=True,\n",
    "                   batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'], label='training')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mse loss')\n",
    "plt.legend()\n",
    "plt.title('MSE loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training \n",
    "crop = 100 \n",
    "\n",
    "train_small = model.predict(train_input[0:100,:])\n",
    "output_small = train_output[0:100,:]\n",
    "\n",
    "def comparison_plot(compare, true):\n",
    "    plt.figure(figsize=(8,8*output_length))\n",
    "    for i in range(0,output_length):\n",
    "        plt.subplot(output_length,1,i+1)\n",
    "        plt.plot(range(0,crop), compare[:,i], 'bo', markersize=3, label = 'Predictions')\n",
    "        plt.plot(range(0,crop), true[:,i], 'ro', markersize=3, label = 'True Value')\n",
    "        ym, yM = plt.ylim()\n",
    "        for x in range(100):\n",
    "            plt.vlines(x, color='g', linestyle='-', alpha=0.2, ymin= \n",
    "                        min(compare[x,i], true[x,i]), \n",
    "                        ymax= max(compare[x,i], true[x,i]))\n",
    "        plt.hlines(0, xmin=-20, xmax=crop+20, alpha=0.5)\n",
    "        MSE = 1/compare[:,i].size*np.sum((compare[:,i]- true[:,i])**2)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(output_keys[i])\n",
    "        plt.xlim(0, crop)\n",
    "        plt.title(output_keys[i] + \" MSE: \" + str(MSE))\n",
    "        plt.legend()\n",
    "\n",
    "comparison_plot(train_small, output_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "\n",
    "predictions_small = predictions[0:crop,:]\n",
    "test_output_small = test_output[0:crop,:]\n",
    "\n",
    "MSE = 1/predictions.size*np.sum((predictions- test_output)**2)\n",
    "print(\"total MSE: \" + str(MSE))\n",
    "\n",
    "for i in range(output_length):\n",
    "    MSE = 1/predictions[:,i].size*np.sum((predictions[:,i] -test_output[:,i])**2)\n",
    "    print(\"{0} MSE : \".format(output_keys[i]), '%.10f'%MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comparison_plot(predictions_small, test_output_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Back to original units \n",
    "\n",
    "scaled_predictions = predictions*out_std + out_mean\n",
    "scaled_output = test_output*out_std + out_mean\n",
    "\n",
    "comparison_plot(scaled_predictions[0:crop,:], scaled_output[0:crop,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Histograms \n",
    "\n",
    "def histograms(compare, true):\n",
    "    plt.figure(figsize=(8,8*output_length))\n",
    "    for i in range(0, output_length):\n",
    "        plt.subplot(output_length,1,i+1)\n",
    "        histo, bin_edges = np.histogram(compare[:,i],20)\n",
    "        plt.hist(compare[:,i], bin_edges, histtype = 'step', color='b', label='Predictions')\n",
    "        plt.hist(true[:,i], bin_edges, histtype = 'step', color='r', label='True Value')\n",
    "        plt.xlabel(output_keys[i])\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(output_keys[i])\n",
    "        plt.legend()\n",
    "\n",
    "histograms(scaled_predictions, scaled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
