{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Layer, Conv1D, Flatten, Dense, Input, Concatenate, Masking, LSTM, TimeDistributed, Lambda, Reshape, Multiply, BatchNormalization, Bidirectional, Dot, RepeatVector, Add\n",
    "from tensorflow.keras import regularizers \n",
    "from tensorflow.keras import initializers\n",
    "import h5py \n",
    "import os \n",
    "from clr_callback import *\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow.keras.backend as K  \n",
    "from tensorflow.keras.optimizers import * \n",
    "# import keras_one_cycle_clr as ktool\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "dataset = h5py.File('./../../../../../data/hongtao/both-5-8-2020.h5','r')\n",
    "# ['j1_pt', 'j1_eta', 'j1_phi', 'j1_x', 'j1_y', 'j1_z', 'j1_m', 'j1_e', 'j1_DL1r', 'j1_isbtag', 'j2_pt', 'j2_eta', 'j2_phi', 'j2_x', 'j2_y', 'j2_z', 'j2_m', 'j2_e', 'j2_DL1r', 'j2_isbtag', 'j3_pt', 'j3_eta', 'j3_phi', 'j3_x', 'j3_y', 'j3_z', 'j3_m', 'j3_e', 'j3_DL1r', 'j3_isbtag', 'j4_pt', 'j4_eta', 'j4_phi', 'j4_x', 'j4_y', 'j4_z', 'j4_m', 'j4_e', 'j4_DL1r', 'j4_isbtag', 'j5_pt', 'j5_eta', 'j5_phi', 'j5_x', 'j5_y', 'j5_z', 'j5_m', 'j5_e', 'j5_DL1r', 'j5_isbtag', 'j6_pt', 'j6_eta', 'j6_phi', 'j6_x', 'j6_y', 'j6_z', 'j6_m', 'j6_e', 'j6_DL1r', 'j6_isbtag', 'lep_pt', 'lep_eta', 'lep_phi', 'lep_x', 'lep_y', 'lep_z', 'lep_e', 'met_met', 'met_phi']\n",
    "\n",
    "X_keys = ['j1_pt', 'j1_eta', 'j1_phi', 'j1_m', 'j1_DL1r', 'j2_pt', 'j2_eta', 'j2_phi', 'j2_m', 'j2_DL1r', 'j3_pt', 'j3_eta', 'j3_phi', 'j3_m', 'j3_DL1r', 'j4_pt', 'j4_eta', 'j4_phi', 'j4_m', 'j4_DL1r', 'j5_pt', 'j5_eta', 'j5_phi', 'j5_m', 'j5_DL1r', 'j6_pt', 'j6_eta', 'j6_phi', 'j6_m', 'j6_DL1r', 'lep_pt', 'lep_eta', 'lep_phi', 'met_met', 'met_phi']\n",
    "\n",
    "Y_keys = ['th_pt', 'th_eta','th_phi','th_m', 'wh_pt', 'wh_eta', 'wh_phi', 'wh_m', 'tl_pt', 'tl_eta', 'tl_phi', 'tl_m', 'wl_pt', 'wl_eta', 'wl_phi', 'wl_m']\n",
    "phi_keys = list(filter(lambda a: 'phi' in a, dataset.keys()))\n",
    "eta_keys = list(filter(lambda a: 'eta' in a, dataset.keys()))\n",
    "pt_keys =  list(filter(lambda a: 'pt' in a, dataset.keys()))\n",
    "m_keys = list(filter(lambda a: 'm' in a, dataset.keys()))\n",
    "DL1r_keys = list(filter(lambda a: 'DL1r' in a, dataset.keys()))\n",
    "\n",
    "Y_length = len(Y_keys)\n",
    "X_length = len(X_keys)\n",
    "crop0 =  1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max scaling error: 4.547473508864641e-13\n"
     ]
    }
   ],
   "source": [
    "import scale_variables\n",
    "import shape_timesteps\n",
    "\n",
    "# How to scale each input/output\n",
    "def get_methods(keys):\n",
    "    types = {'pt':'cart_pt', 'eta':'meanmax', 'phi':'sincos', 'DL1r':'DL1r', 'm':'meanmax', 'x':'null', 'y':'null', 'z':'null', 'e':'null', 'isbtag':'null','met':'meanmax'}\n",
    "    var_types = [key.split('_')[1] for key in keys]\n",
    "    methods = [types[var] for var in var_types]\n",
    "    return methods\n",
    "\n",
    "X_methods = get_methods(X_keys)\n",
    "Y_methods = get_methods(Y_keys)\n",
    "\n",
    "Scaler = scale_variables.Scale_variables()\n",
    "(X_total, X_maxmean), X_names = Scaler.scale_arrays(X_keys, X_methods, True)\n",
    "(Y_total, Y_maxmean), Y_names = Scaler.scale_arrays(Y_keys, Y_methods, True)\n",
    "\n",
    "error = Scaler.test_inverse(Y_keys, Y_methods, True)\n",
    "print('Max scaling error: {}'.format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and test array\n",
    "split = int(np.floor(0.9*crop0)) # 80/20 split \n",
    "\n",
    "trainY, testY = Y_total[0:split,:], Y_total[split:,:]\n",
    "\n",
    "timestep_builder = shape_timesteps.Shape_timesteps()\n",
    "totalX_jets, totalX_other = timestep_builder.reshape_X(X_total, X_names, False,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 2 1 0 1 3 2 1 0 2 3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "def random_choice_noreplace(m,n, axis=-1):\n",
    "    # m, n are the number of rows, cols of output\n",
    "    return np.array(np.random.rand(m,n).argsort(axis=axis))\n",
    "\n",
    "def random_shuffle_jets(jets):\n",
    "    m,n = jets.shape[0], jets.shape[1]\n",
    "    idx = random_choice_noreplace(m,n,axis=1).reshape((m,n,1))\n",
    "    return np.take_along_axis(jets,idx,axis=1)\n",
    "\n",
    "def random_permutation_matrix(jets):\n",
    "    m,n = jets.shape[0], jets.shape[1]\n",
    "    ind = random_choice_noreplace(m,n,axis=1).flatten()\n",
    "    fast = np.tile(np.arange(0,n),m).flatten()\n",
    "    slow = np.repeat(np.arange(0,n),m).flatten()\n",
    "    indx = np.stack([slow,fast,ind])\n",
    "    zeros = np.zeroes((m,n,n))\n",
    "    zeros[indx] = 1\n",
    "    return zeros \n",
    "    \n",
    "    \n",
    "# idx = random_choice_noreplace(3,4,axis=1) #.reshape((3,4,1))\n",
    "# print(idx.flatten())\n",
    "# id1 = np.array([[0,1,2,3],[0,1,2,3],[0,1,2,3]])\n",
    "# print(id1)\n",
    "\n",
    "# x = np.array([[[1,2,3],[4,5,6],[7,8,9]]])\n",
    "# idx = [[0,0,0],[0,1,2], [0,2,1]]\n",
    "# print(x.shape, y.shape)\n",
    "# print(x[tuple(idx)])\n",
    "\n",
    "\n",
    "# a = np.array([[[1,8], [9,5], [4,5], [3,3]],\n",
    "#        [[4,0], [7,8], [3,1], [5,2]],\n",
    "#        [[7,5], [4,9], [4,2], [9,4]]])\n",
    "\n",
    "# # print(a.shape)\n",
    "# \n",
    "# print(np.take_along_axis(a,idx,axis=1))\n",
    "# print(np.take(a,idx,axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# DL1r = totalX_jets[:,:,-1]\n",
    "# argDL1r = np.argsort(DL1r,axis=-1)\n",
    "# argDL1r = argDL1r.reshape((argDL1r.shape[0], -1, 1))\n",
    "# jets_byDL1r = np.take_along_axis(totalX_jets, argDL1r, axis=1)\n",
    "# # totalX_jets = np.concatenate([totalX_jets, jets_byDL1r], axis=1)\n",
    "\n",
    "# totalX_jets == jets_byDL1r\n",
    "\n",
    "totalY_jets = totalX_jets\n",
    "totalX_jets = random_shuffle_jets(totalX_jets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_jets, testX_jets = totalX_jets[0:split,:,:], totalX_jets[split:,:,:]\n",
    "trainX_other, testX_other = totalX_other[0:split,:], totalX_other[split:,:]\n",
    "trainY_jets, testY_jets = totalY_jets[0:split,:,:], totalY_jets[split:,:,:]\n",
    "\n",
    "# Save some memory \n",
    "del totalX_jets\n",
    "del totalX_other \n",
    "\n",
    "print(trainX_jets.shape, trainX_other.shape, trainY_jets.shape)\n",
    "print(testX_jets.shape, testX_other.shape, testY_jets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of total  Y variables \n",
    "show = False\n",
    "\n",
    "other_bins = np.linspace(-1, 1, 40)\n",
    "phi_bins = np.linspace(-1, 1, 40)\n",
    "pt_bins = np.linspace(-1, 1, 40)\n",
    "Y_bins = [phi_bins if 'phi' in name else pt_bins if 'pt' in name else other_bins for name in Y_names]\n",
    "\n",
    "if show:\n",
    "    plt.figure(figsize=(6,6*trainY.shape[1]))\n",
    "    for i in range(0, trainY.shape[1]):\n",
    "        plt.subplot(trainY.shape[1], 1, i+1)\n",
    "        bins = Y_bins[i]\n",
    "        plt.hist(Y_total[:,i], bins, histtype='step')\n",
    "        plt.xlabel(Y_names[i])\n",
    "        plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Histograms of X variables (without the mask)\n",
    "show = False\n",
    "\n",
    "if show:\n",
    "    plt.figure(figsize=(6,6*X_total.shape[1]))\n",
    "    for i in range(0, X_total.shape[1]):\n",
    "        plt.subplot(X_total.shape[1], 1, i+1)\n",
    "        plt.hist(X_total[:,i], 40, histtype='step')\n",
    "        plt.xlabel(X_names[i])\n",
    "        plt.ylabel('Frequency')\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinkhornLayer(Layer):\n",
    "    def __init__(self, n_iters=21, temperature=0.01, **kwargs):\n",
    "        self.supports_masking = False\n",
    "        self.n_iters = n_iters\n",
    "        self.temperature = K.constant(temperature)\n",
    "        super(SinkhornLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, input_tensor, mask=None):\n",
    "        input_shape = tf.shape(input_tensor)\n",
    "        n = K.shape(input_tensor)[1]\n",
    "        log_alpha = K.reshape(input_tensor, [-1, n, n])\n",
    "        log_alpha /= self.temperature\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            log_alpha -= K.reshape(K.log(K.sum(K.exp(log_alpha), axis=2)), [-1, n, 1])\n",
    "            log_alpha -= K.reshape(K.log(K.sum(K.exp(log_alpha), axis=1)), [-1, 1, n])\n",
    "        return tf.reshape(K.exp(log_alpha), self.compute_output_shape(input_shape)) \n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        return None\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "\n",
    "\n",
    "max_len = trainX_jets.shape[1]\n",
    "def build_classifier():\n",
    "    jet_input = Input(shape=(trainX_jets.shape[1], trainX_jets.shape[2]))\n",
    "#     other_input = Input(shape=(trainX_other.shape[1]))\n",
    "#     flat_jets =  Flatten()(jet_input)\n",
    "#     concat0 = concatenate([other_input, flat_jets])\n",
    "    \n",
    "    embedding = TimeDistributed(Dense(units=256))(jet_input)\n",
    "    # pairwise scalar products to take object 'interactions' into account\n",
    "    dot = Dot([-1, -1])([embedding, embedding])\n",
    "\n",
    "    # reshaping into a single vector\n",
    "    interactions = Reshape(target_shape=(trainX_jets.shape[1] * trainX_jets.shape[2],))(dot)\n",
    "\n",
    "    # two independent fully-connected layers with different activations\n",
    "    interactions1 = Dense(units=max_len * max_len, activation=\"sigmoid\")(interactions)\n",
    "    interactions2 = Dense(units=max_len * max_len, activation=\"tanh\")(interactions)\n",
    "\n",
    "    # (this trick seems to be an important one)\n",
    "    added_interactions = Add()([interactions1, interactions2])\n",
    "\n",
    "    # appending 'interactions' to embeddings\n",
    "    interactions_replicated = RepeatVector(max_len)(added_interactions)\n",
    "    joined = Concatenate(axis=-1)([embedding, interactions_replicated])\n",
    "\n",
    "    # dense layer for dense layer outputs of the size equal to length\n",
    "    layer_for_combining = TimeDistributed(Dense(units=max_len, activation=\"tanh\", ),\n",
    "                                          input_shape=(max_len, max_len ** 2 + max_len))(joined)\n",
    "\n",
    "    # permutation approximation layer\n",
    "    sinkhorn = SinkhornLayer(n_iters=20, temperature=0.03, name=\"sinkhorn\")(layer_for_combining)\n",
    "    permute_apply = Dot(axes=[-2, -2])([sinkhorn, jet_input])\n",
    "    \n",
    "    model = keras.models.Model(inputs=[jet_input], outputs=sinkhorn)\n",
    "    lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate=1e-3, decay_steps=10000,end_learning_rate=1e-4,power=0.25)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    model.compile(loss='mse', optimizer= optimizer, metrics=['mse'])\n",
    "    return model \n",
    "\n",
    "# model = keras.models.load_model('Jet_Reweight_cartpt.keras')\n",
    "model = build_classifier()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6, verbose=1)\n",
    "\n",
    "clr_triangular = CyclicLR(base_lr=1e-5, max_lr=1e-4, step_size=5000, mode='exp_range',gamma=0.9999)\n",
    "# class_weight = {0:2, 1:1, 2:1, 3:1, 4:1, 5:2, 6:1, 7:1, 8:1, 9:1, 10:2, 11:1, 12:1, 13:1, 14:1, 15:2, 16:1, 17:1, 18:1, 19:1}\n",
    "\n",
    "\n",
    "Epochs= 64\n",
    "\n",
    "# del X_total\n",
    "# del Y_total\n",
    "# def loss(true, pred):\n",
    "#     return K.mean(K.square(true-pred)*(1+K.square(true)))\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit([trainX_jets], trainY_jets, verbose=1, epochs=Epochs,\n",
    "                   validation_data=([testX_jets], testY_jets), shuffle=True, callbacks=[early_stop],\n",
    "                    batch_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Fit model\n",
    "\n",
    "# early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
    "# # reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# clr_triangular = CyclicLR(base_lr=1e-5, max_lr=1e-4, step_size=5000, mode='exp_range',gamma=0.9999)\n",
    "# # class_weight = {0:2, 1:1, 2:1, 3:1, 4:1, 5:2, 6:1, 7:1, 8:1, 9:1, 10:2, 11:1, 12:1, 13:1, 14:1, 15:2, 16:1, 17:1, 18:1, 19:1}\n",
    "\n",
    "\n",
    "# Epochs= 256\n",
    "\n",
    "# # del X_total\n",
    "# # del Y_total\n",
    "# # def loss(true, pred):\n",
    "# #     return K.mean(K.square(true-pred)*(1+K.square(true)))\n",
    "\n",
    "\n",
    "\n",
    "# history = model.fit([trainX_jets, trainX_other], trainY, verbose=1, epochs=Epochs,\n",
    "#                    validation_data=([testX_jets, testX_other], testY), shuffle=True, callbacks=[early_stop],\n",
    "#                     batch_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.xlabel('Training Iterations')\n",
    "# plt.ylabel('Learning Rate')\n",
    "# plt.title(\"CLR - 'triangular' Policy\")\n",
    "# plt.plot(clr_triangular.history['iterations'], clr_triangular.history['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'], label='training')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mse loss')\n",
    "plt.legend()\n",
    "plt.title('MSE loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_unscaled = model.predict([testX_jets]).reshape((-1,36))\n",
    "true_unscaled = testY_jets.reshape((-1,36))\n",
    "\n",
    "\n",
    "\n",
    "# total_predictions = model.predict([np.append(trainX_jets,testX_jets,axis=0), np.append(trainX_other,testX_other,axis=0)])\n",
    "# (Y_total, TO_maxmean0), _ = Scaler.scale_arrays(Y_keys, Y_methods, True)\n",
    "\n",
    "# predictions_origscale = Scaler.invscale_arrays(Y_keys, total_predictions, _, Y_methods, TO_maxmean0)[split:,:]\n",
    "# true_origscale = Scaler.invscale_arrays(Y_keys, Y_total, _, Y_methods, TO_maxmean0)[split:,:]\n",
    "\n",
    "# del Y_total\n",
    "# del TO_maxmean0\n",
    "# del _\n",
    "# del total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_unscaled.shape, true_unscaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('true', predictions_origscale)\n",
    "# np.save('pred', true_origscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training scale plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis\n",
    "jet_names = list(filter(lambda a: 'j' in a, X_names))\n",
    "Analysis = analysis.Analysis \n",
    "Analysis.display_errors(predictions_unscaled, true_unscaled, jet_names, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis.display_errors(predictions_origscale, true_origscale, Y_keys, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Analysis.predictions_vs_sample(predictions_unscaled, true_unscaled, jet_names, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analysis.variable_histogram(predictions_unscaled, true_unscaled, jet_names, False, Y_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis.difference_histogram(predictions_unscaled, true_unscaled, Y_names, False, Y_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analysis.predicted_vs_true(predictions_unscaled, true_unscaled, jet_names, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    " # Original scale plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Analysis.display_errors(predictions_origscale, true_origscale, Y_keys, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analysis.predictions_vs_sample(predictions_origscale, true_origscale, Y_keys, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analysis.variable_histogram(predictions_origscale, true_origscale, Y_keys, True, [None for name in Y_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Analysis.predicted_vs_true(predictions_origscale, true_origscale, Y_keys, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('Jet_Reweight_lr_decay_ex.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import observables \n",
    "import importlib\n",
    "importlib.reload(observables)\n",
    "truths = observables.fill_observables(true_origscale, True, Y_keys)\n",
    "preds = observables.fill_observables(predictions_origscale, False, Y_keys)\n",
    "\n",
    "# top_dphi=np.abs(th_phi-tl_phi)\n",
    "plt.figure(figsize=(12,6))\n",
    "observables.plot_hist(truths['top_dphi'], preds['top_dphi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# top_m0= th_m**2-th_p**2 + tl_m**2-tl_p**2\n",
    "plt.figure(figsize=(12,6))\n",
    "observables.plot_hist(truths['top_m0'],preds['top_m0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eta_cm=0.5*(th_eta-tl_eta), eta_boost=0.5*(th_eta+tl_eta)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(211)\n",
    "observables.plot_hist(truths['eta_cm'], preds['eta_cm'])\n",
    "plt.subplot(212)\n",
    "observables.plot_hist(truths['eta_boost'], preds['eta_boost'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# th_Pout=dot(th_P, cross(tl_P,ez)/norm(tl_P,ez)), tl_Pout=dot(tl_P, cross(th_P,ez)/norm(th_P,ez))\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(211)\n",
    "observables.plot_hist(truths['th_Pout'], preds['th_Pout'])\n",
    "plt.subplot(212)\n",
    "observables.plot_hist(truths['tl_Pout'], preds['tl_Pout'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt_tot = th_pt+tl_pt\n",
    "plt.figure(figsize=(12,6))\n",
    "observables.plot_hist(truths['pt_tot'],preds['pt_tot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('Jet_Reweight_cartpt.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('aug_9/Jet_Reweight_cartpt_pred', predictions_origscale)\n",
    "np.save('aug_9/Jet_Reweight_cartpt_true', true_origscale)\n",
    "truth_observables = [truths[list(truths.keys())[i]].value for i in range(len(list(truths.keys())))]\n",
    "pred_observables = [preds[list(preds.keys())[i]].value for i in range(len(list(preds.keys())))]\n",
    "truth_observables = np.stack(truth_observables, axis=1)\n",
    "truth_observables = truth_observables.reshape((truth_observables.shape[0], -1))\n",
    "pred_observables = np.stack(pred_observables, axis=1)\n",
    "pred_observables = pred_observables.reshape((pred_observables.shape[0], -1))\n",
    "\n",
    "np.save('aug_9/Jet_Reweight_cartpt_pred_observables', pred_observables)\n",
    "np.save('aug_9/Jet_Reweight_cartpt_true_observables', truth_observables)\n",
    "print(list(preds.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
