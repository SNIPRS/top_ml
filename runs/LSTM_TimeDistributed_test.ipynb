{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers \n",
    "from tensorflow.keras.utils import plot_model\n",
    "import scipy.stats as stats\n",
    "import os \n",
    "import h5py\n",
    "import sys\n",
    "import re\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TimeDistributed_test\"\n",
    "\n",
    "be = h5py.File('./../../../../../data/hongtao/variables_tt_re.h5','r')\n",
    "bmu = h5py.File('./../../../../../data/hongtao/variables_tt_rmu.h5','r')\n",
    "be1 = h5py.File('./../../../../../data/hongtao/variables_tt_re1.h5','r')\n",
    "print(be.keys())\n",
    "print('')\n",
    "print(be1.keys())\n",
    "dataset = be1\n",
    "\n",
    "input_keys = ['j1_pt', 'j1_eta', 'j1_phi', 'j1_m', 'j1_DL1r', 'j2_pt', 'j2_eta', 'j2_phi', 'j2_m', 'j2_DL1r', 'j3_pt', 'j3_eta', 'j3_phi', 'j3_m', 'j3_DL1r', 'j4_pt', 'j4_eta', 'j4_phi', 'j4_m', 'j4_DL1r', 'j5_pt', 'j5_eta', 'j5_phi', 'j5_m', 'j5_DL1r', 'j6_pt', 'j6_eta', 'j6_phi', 'j6_m', 'j6_DL1r', 'lep_pt', 'lep_eta', 'lep_phi', 'met_met', 'met_phi'] \n",
    "input_jetkeys = ['j1_pt', 'j1_eta', 'j1_phi', 'j1_m', 'j1_DL1r', 'j2_pt', 'j2_eta', 'j2_phi', 'j2_m', 'j2_DL1r', 'j3_pt', 'j3_eta', 'j3_phi', 'j3_m', 'j3_DL1r', 'j4_pt', 'j4_eta', 'j4_phi', 'j4_m', 'j4_DL1r', 'j5_pt', 'j5_eta', 'j5_phi', 'j5_m', 'j5_DL1r', 'j6_pt', 'j6_eta', 'j6_phi', 'j6_m', 'j6_DL1r']\n",
    "input_otherkeys = ['lep_pt', 'lep_eta', 'lep_phi', 'met_met', 'met_phi']\n",
    "\n",
    "output_keys = ['th_pt', 'th_eta','th_phi', 'tl_pt', 'tl_eta', 'tl_phi', 'wl_eta', 'wl_phi', 'wl_pt']\n",
    "phi_keys = ['j1_phi', 'j2_phi', 'j3_phi','j4_phi','j5_phi','j6_phi', 'lep_phi', 'met_phi', 'th_phi', 'tl_phi', 'wl_phi']\n",
    "eta_keys = ['j1_eta', 'j2_eta', 'j3_eta', 'j4_eta', 'j5_eta', 'j5_phi', 'j6_eta', 'lep_eta', 'th_eta', 'tl_eta', 'wl_eta']\n",
    "pt_keys = ['j1_pt', 'j2_pt','j3_pt','j4_pt','j5_pt','j6_pt','lep_pt','th_pt', 'tl_pt', 'wl_pt']\n",
    "m_keys = ['j1_m','j2_m', 'j3_m', 'j4_m', 'j5_m', 'j6_m']\n",
    "DL1r_keys = ['j1_DL1r','j2_DL1r','j3_DL1r','j4_DL1r','j5_DL1r','j6_DL1r']\n",
    "\n",
    "num_jets = 6 \n",
    "output_length = len(output_keys)\n",
    "input_length = len(input_keys)\n",
    "crop0 =  100000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lep_phi = np.array(dataset.get('lep_phi'))[0:crop0]\n",
    "\n",
    "def phi_transform(arr, max0, mean):\n",
    "    arr = (arr-mean)\n",
    "    arr = arr/max0/1.01/2+0.5\n",
    "    z = stats.norm.ppf(arr)/2.5\n",
    "    return z \n",
    "\n",
    "def invphi_transform(z, max0, mean):\n",
    "    arr = stats.norm.cdf(2.5*z)\n",
    "    arr = (arr-0.5)*max0*1.01*2+mean\n",
    "    return arr \n",
    "\n",
    "def jet_existence_dict():\n",
    "    dic = {}\n",
    "    for key in phi_keys:\n",
    "        variable = key.split('_')[0]\n",
    "        if bool(re.match('^j[0-9]+$', variable)): # If the variable is a jet\n",
    "            v = np.array(dataset.get(variable + '_pt'))[0:crop0]\n",
    "            dic[key] = (v>1)*1\n",
    "        else:\n",
    "            dic[key] = np.ones(crop0, dtype=int)\n",
    "    return dic\n",
    "        \n",
    "def phi1_transform(arr, max0, mean, exist):\n",
    "    w = (arr - lep_phi*exist) % (2*np.pi)\n",
    "    x = w - 2*np.pi*(w>np.pi)\n",
    "    y = x - (1-exist)*np.pi*20\n",
    "    y = y-mean\n",
    "    z = y/max0\n",
    "    return z\n",
    "\n",
    "def invphi1_transform(z, max0, mean, exist):\n",
    "    y = z*max0+mean\n",
    "    x = y+(1-exist)*np.pi*20\n",
    "    w = x + 2*np.pi*(x<0)\n",
    "    arr = (w + lep_phi*exist) % (2*np.pi)\n",
    "    arr = arr - 2*np.pi*(arr > np.pi)\n",
    "    return arr \n",
    "\n",
    "\n",
    "def phi2_transform(arr, max0, mean, exist):\n",
    "    w = (arr - lep_phi*exist) % (2*np.pi)\n",
    "    # x = w - 2*np.pi*(w>np.pi)\n",
    "    y = w - (1-exist)*20\n",
    "    z = y/(np.pi)\n",
    "    return z\n",
    "\n",
    "def invphi2_transform(z, max0, mean, exist):\n",
    "    y = z*np.pi\n",
    "    x = y+(1-exist)*20\n",
    "    # w = x + 2*np.pi*(x<0)\n",
    "    arr = (x + lep_phi*exist) % (2*np.pi)\n",
    "    arr = arr - 2*np.pi*(arr > np.pi)\n",
    "    return arr \n",
    "\n",
    "\n",
    "def pt_transform(arr, max0, mean):\n",
    "    return arr/max0\n",
    "\n",
    "def invpt_transform(z, max0, mean):\n",
    "    return z*max0 \n",
    "\n",
    "def meanmax_transform(arr, max0, mean):\n",
    "    arr = arr-mean\n",
    "    z = arr/max0\n",
    "    return z\n",
    "\n",
    "def invmeanmax_transform(z, max0, mean):\n",
    "    return z*max0+mean\n",
    "\n",
    "def get_maxmean_dict(): \n",
    "    to_get = [pt_keys, eta_keys, m_keys, DL1r_keys]\n",
    "    keys = ['pt', 'eta', 'm','DL1r']\n",
    "    maxmean= {} \n",
    "    \n",
    "    for i in range(4):\n",
    "        dset = to_get[i]\n",
    "        for x in dset:\n",
    "            arr = []\n",
    "            arr.append(np.array(dataset.get(x))[0:crop0])\n",
    "        arr = np.stack(arr,axis=1)\n",
    "        maxmean[keys[i]] = (np.max(np.abs(arr)), np.mean(arr))\n",
    "    \n",
    "    maxmean['phi'] = (np.pi, 0)\n",
    "    maxmean['met'] = (np.max(np.abs(dataset.get('met_met'))), np.mean(dataset.get('met_met')))\n",
    "    return maxmean \n",
    "    \n",
    "def scale_arrays(keys, maxmean_dict):\n",
    "    exist_dict = jet_existence_dict()\n",
    "    lep_phi = np.array(dataset.get('lep_phi'))[0:crop0]\n",
    "    \n",
    "    arrays = []\n",
    "    for key in keys:\n",
    "        var = np.array(dataset.get(key))[0:crop0]\n",
    "        if key in phi_keys:\n",
    "            max0, mean = maxmean_dict['phi']\n",
    "            exist = exist_dict[key]\n",
    "            if key == 'tl_phi' or key == 'wl_phi':\n",
    "                z = phi1_transform(var, max0, mean, exist) \n",
    "            else:\n",
    "                z = phi2_transform(var, max0, mean, exist)\n",
    "        elif key in pt_keys:\n",
    "            max0, mean = maxmean_dict['pt']\n",
    "            z = pt_transform(var, max0, mean)\n",
    "        elif key in m_keys:\n",
    "            max0, mean = maxmean_dict['m']\n",
    "            z = pt_transform(var, max0, mean)\n",
    "        else:\n",
    "            max0, mean = maxmean_dict[key.split('_')[1]]\n",
    "            z = meanmax_transform(var, max0, mean)\n",
    "        arrays.append(z)\n",
    "    arrays = np.stack(arrays, axis=1)\n",
    "    return arrays\n",
    "\n",
    "def invscale_arrays(keys, arrays, maxmean_dict):\n",
    "    exist_dict = jet_existence_dict()\n",
    "\n",
    "    total = []\n",
    "    for i in range(arrays.shape[1]):\n",
    "        z=arrays[:,i]\n",
    "        full_key = keys[i]\n",
    "        key = keys[i].split('_')[1]\n",
    "        if key == 'pt':\n",
    "            max0, mean = maxmean_dict[key]\n",
    "            total.append(invpt_transform(z, max0, mean))\n",
    "        elif key=='phi':\n",
    "            max0, mean = maxmean_dict[key]\n",
    "            exist = exist_dict[full_key]\n",
    "            if full_key == 'tl_phi' or full_key == 'wl_phi':\n",
    "                total.append(invphi1_transform(z, max0, mean, exist))\n",
    "            else:\n",
    "                total.append(invphi2_transform(z, max0, mean, exist))\n",
    "        elif key=='m':\n",
    "            max0, mean = maxmean_dict[key]\n",
    "            total.append(invpt_transform(z, max0, mean))\n",
    "        else:\n",
    "            max0, mean = maxmean_dict[key]\n",
    "            total.append(invmeanmax_transform(z, max0, mean))\n",
    "            \n",
    "    return np.stack(total,axis=1) \n",
    "\n",
    "def create_mask():\n",
    "    exist = jet_existence_dict()\n",
    "    mask = [exist[list(exist.keys())[i]] for i in range(num_jets)] \n",
    "    samples_jets = np.stack(mask,axis=1)\n",
    "    samples_jets = samples_jets.reshape((samples_jets.shape[0], samples_jets.shape[1], 1))\n",
    "    return np.repeat(samples_jets, 5, axis=2) # 5 feature mask\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into numpy arrays and mean normalize\n",
    "\n",
    "maxmean = get_maxmean_dict()\n",
    "total_input = scale_arrays(input_keys, maxmean)\n",
    "\n",
    "total_output = scale_arrays(output_keys, maxmean)\n",
    "out_scaled = invscale_arrays(output_keys, total_output, maxmean)\n",
    "\n",
    "split = int(np.floor(0.8*crop0)) # 80/20 split \n",
    "shuffle = False\n",
    "\n",
    "if shuffle:\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(total_input)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(total_output)\n",
    "\n",
    "# Split into jets and other input \n",
    "total_input_jets, total_input_other = total_input[:, 0:len(input_jetkeys)], total_input[:, len(input_jetkeys):]\n",
    "\n",
    "# Reshape jets to be of shape (samples, num_jets, features per jet)\n",
    "total_input_jets = np.split(total_input_jets, num_jets, axis=1)\n",
    "total_input_jets = np.stack(total_input_jets, axis=1) \n",
    "\n",
    "# Mask the non-existent jets with value=-1\n",
    "mask = create_mask()\n",
    "total_input_jets = total_input_jets*mask + (1-mask)*(-1) \n",
    "\n",
    "\n",
    "# Split into training and validation data \n",
    "train_input_jets, test_input_jets = total_input_jets[0:split, :, :], total_input_jets[split:, :, :]\n",
    "train_input_other, test_input_other = total_input_other[0:split, :], total_input_other[split:, :]\n",
    "train_output, test_output = total_output[0:split, :], total_output[split:, :]\n",
    "\n",
    "print(train_input_jets.shape, train_input_other.shape)\n",
    "print(test_input_jets.shape, test_input_other.shape)\n",
    "print(train_output.shape, test_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_input_jets():\n",
    "    features_perjet = 5\n",
    "    features = ['pt', 'eta', 'phi', 'm', 'DL1r']\n",
    "    colours = ['r', 'g', 'b', 'm', 'c', 'orange', 'purple', 'k']\n",
    "    plt.figure(figsize=(8, 8*features_perjet))\n",
    "    for i in range(features_perjet):\n",
    "        plt.subplot(features_perjet, 1, i+1)\n",
    "        plt.xlabel(features[i])\n",
    "        plt.title(features[i])\n",
    "        plt.ylabel('Frequency')\n",
    "        for j in range(num_jets):\n",
    "            plt.hist(train_input_jets[:,j,i], 20, \n",
    "                     histtype='step', color=colours[j], \n",
    "                     density=True, label='jet ' + str(j+1))\n",
    "        plt.legend()\n",
    "        \n",
    "if True: \n",
    "    plot_input_jets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_input_other():\n",
    "    num_plots = train_input_other.shape[1]\n",
    "    plt.figure(fisize=(8, 8*num_plots))\n",
    "    for i in range(num_plots):\n",
    "        plt.subplot(num_plots, 1, i+1)\n",
    "        var = train_input_other[:, i]\n",
    "        plt.xlabel(\"{} {}\".format(np.min(var), np.max(var)))\n",
    "        plt.title(input_otherkeys[i])\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.histogram(var, 20, histtype='step', color='b', density=True)\n",
    "        \n",
    "if True:\n",
    "    plot_input_jets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outputs():\n",
    "    num_plots = output_keys.shape[1]\n",
    "    plt.figure(figsize=(8, 8*num_plots))\n",
    "    for i in range(num_plots):\n",
    "        plt.subplot(num_plots, 1, i+1)\n",
    "        var = train_output[:, i]\n",
    "        plt.xlabel(\"{} {}\".format(np.min(var), np.max(var)))\n",
    "        plt.title(output_keys[i])\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.histogram(var, 20, histtype='step', color='b', density=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "\n",
    "def build_model():\n",
    "    jet_input = layers.Input(shape=(num_jets, 5))\n",
    "    masking = layers.Masking(mask_value=-10)(jet_input)\n",
    "    LSTM1 = layers.LSTM(64, return_sequences=True)(masking)\n",
    "    BN11 = layers.BatchNormalization()(LSTM1)\n",
    "    TD1 = layers.TimeDistributed(layers.Dense(64, activation='relu'))(BN11)\n",
    "    TD2 = layers.TimeDistributed(layers.Dense(32, activation = 'relu'))(TD1)\n",
    "    flat_jet = layers.Flatten()(TD2)\n",
    "    other_input = layers.Input(shape=(train_input_other.shape[1]))\n",
    "    dense21 = layers.Dense(64, activation='relu')(other_input)\n",
    "    dense22 = layers.Dense(64, activation='relu')(dense21)\n",
    "    flat_other = layers.Flatten()(dense22)\n",
    "    concat1 = layers.concatenate([flat_jet, flat_other])\n",
    "    BN1 = layers.BatchNormalization()(concat1)\n",
    "    dense1 = layers.Dense(128, activation='relu')(BN1)\n",
    "    dense2 = layers.Dense(128, activation='relu')(dense1)\n",
    "    dense3 = layers.Dense(128, activation='relu')(dense2)\n",
    "    dense4 = layers.Dense(128, activation='relu')(dense3)\n",
    "    output = layers.Dense(len(output_keys), activation='linear')(dense4)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[jet_input, other_input], outputs=output)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=5e-5)\n",
    "\n",
    "    model.compile(loss='mse', optimizer= optimizer, metrics=['mse'])\n",
    "    return model \n",
    "\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit model\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "Epochs= 1024\n",
    "tf.config.experimental_run_functions_eagerly(True) # I don't know what this does but it fixes one of my errors \n",
    "\n",
    "history = model.fit([train_input_jets, train_input_other] , train_output, verbose=1, epochs=Epochs, \n",
    "                   validation_split=0.2, shuffle=True, callbacks=[callback],\n",
    "                   batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'], label='training')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mse loss')\n",
    "plt.legend()\n",
    "plt.title('MSE loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([test_input_jets, test_input_other])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Training \n",
    "crop = 100 \n",
    "\n",
    "train_small = model.predict([train_input_jets, train_input_other])[0:100,:]\n",
    "output_small = train_output[0:100,:]\n",
    "\n",
    "def comparison_plot(compare, true):\n",
    "    plt.figure(figsize=(8,8*output_length))\n",
    "    for i in range(0,output_length):\n",
    "        plt.subplot(output_length,1,i+1)\n",
    "        plt.plot(range(0,crop), compare[:,i], 'bo', markersize=3, label = 'Predictions')\n",
    "        plt.plot(range(0,crop), true[:,i], 'ro', markersize=3, label = 'True Value')\n",
    "        ym, yM = plt.ylim()\n",
    "        for x in range(100):\n",
    "            plt.vlines(x, color='g', linestyle='-', alpha=0.2, ymin= \n",
    "                        min(compare[x,i], true[x,i]), \n",
    "                        ymax= max(compare[x,i], true[x,i]))\n",
    "        plt.hlines(np.mean(true[:,i]), xmin=-20, xmax=crop+20, alpha=0.5)\n",
    "        MSE = 1/compare[:,i].size*np.sum((compare[:,i]- true[:,i])**2)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(output_keys[i])\n",
    "        plt.xlim(0, crop)\n",
    "        plt.title(output_keys[i] + \" MSE: \" + str(MSE))\n",
    "        plt.legend()\n",
    "\n",
    "comparison_plot(train_small, output_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "\n",
    "predictions_small = predictions[0:100,:]\n",
    "test_output_small = test_output[0:100,:]\n",
    "\n",
    "MSE = 1/predictions.size*np.sum((predictions- test_output)**2)\n",
    "print(\"total MSE: \" + str(MSE))\n",
    "\n",
    "for i in range(output_length):\n",
    "    MSE = 1/predictions[:,i].size*np.sum((predictions[:,i] -test_output[:,i])**2)\n",
    "    print(\"{0} MSE : \".format(output_keys[i]), '%.10f'%MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unscaled Histograms \n",
    "\n",
    "plt.figure(figsize=(8,8*output_length))\n",
    "for i in range(len(output_keys)):\n",
    "    plt.subplot(output_length, 1, i+1)\n",
    "    hist0, bin_edges = np.histogram(test_output[:, i], bins=20)\n",
    "    plt.hist(test_output[:,i], bin_edges, histtype='step', color='b', label='true values', density=True)\n",
    "    plt.hist(predictions[:,i], bin_edges, histtype='step', color='r', label='predictions', density=True)\n",
    "    plt.xlabel(output_keys[i] +\" \" +str(np.max(predictions[:,i])) + \" \" + str(np.min(predictions[:,i])))\n",
    "    plt.legend()\n",
    "    plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_output = scale_arrays(output_keys, maxmean)\n",
    "out_scaled = invscale_arrays(output_keys, total_output, maxmean)[split:,:]\n",
    "\n",
    "total_predictions = model.predict([np.append(train_input_jets,test_input_jets, axis=0), np.append(train_input_other,test_input_other, axis=0)])\n",
    "predict_scaled = invscale_arrays(output_keys, total_predictions, maxmean)[split:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms to the original scale \n",
    "\n",
    "# out_scaled = invscale_arrays(output_keys, out_max0, out_mean, total_output)\n",
    "# out_scaled = invscale_arrays(output_keys, test_output, maxmean)\n",
    "# predict_scaled = invscale_arrays(output_keys, out_max0, out_mean, total_predictions)[split:,:]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,8*output_length))\n",
    "for i in range(len(output_keys)):\n",
    "    plt.subplot(output_length, 1, i+1)\n",
    "    # hist0, bin_edges = np.histogram(out_scaled[:, i], bins=20)\n",
    "    plt.hist(out_scaled[:, i], 20, histtype='step', color='b', label='true values', density=True)\n",
    "    plt.hist(predict_scaled[:, i], 20, histtype='step', color='r', label='predictions', density=True)\n",
    "    plt.xlabel(output_keys[i])\n",
    "    plt.legend()\n",
    "    plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(output_length):\n",
    "    if output_keys[i].split(\"_\")[1] == 'phi':\n",
    "        mod_pi = (predict_scaled[:,i] - out_scaled[:,i]) % (2*np.pi)\n",
    "        mod_pi = mod_pi - 2*np.pi*(mod_pi > np.pi)\n",
    "        MSE = 1/predict_scaled[:,i].size*np.sum((mod_pi)**2)\n",
    "    else:\n",
    "        MSE = 1/predict_scaled[:,i].size*np.sum((predict_scaled[:,i] - out_scaled[:,i])**2)\n",
    "    print(\"Original Scale {0} MSE : \".format(output_keys[i]), '%.10f'%MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8*output_length))\n",
    "for i in range(output_length):\n",
    "    plt.subplot(output_length, 1, i+1)\n",
    "    # hist0, bin_edges = np.histogram(out_scaled[:, i], bins=20)\n",
    "    plt.plot(out_scaled[:, i], predict_scaled[:, i]-out_scaled[:, i], 'o', color='purple', label='True - Predicted', markersize=2)\n",
    "    plt.xlabel('True: ' + output_keys[i])\n",
    "    plt.legend()\n",
    "    plt.ylabel('Difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
