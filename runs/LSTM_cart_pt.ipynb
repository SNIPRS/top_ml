{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Input, concatenate, Masking, LSTM, TimeDistributed\n",
    "from tensorflow.keras import regularizers \n",
    "from tensorflow.keras import initializers\n",
    "import h5py \n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "dataset = h5py.File('./../../../../../data/hongtao/variables_tt_re2.h5','r')\n",
    "\n",
    "X_keys = ['j1_pt', 'j1_eta', 'j1_phi', 'j1_m', 'j1_DL1r', 'j2_pt', 'j2_eta', 'j2_phi', 'j2_m', 'j2_DL1r', 'j3_pt', 'j3_eta', 'j3_phi', 'j3_m', 'j3_DL1r', 'j4_pt', 'j4_eta', 'j4_phi', 'j4_m', 'j4_DL1r', 'j5_pt', 'j5_eta', 'j5_phi', 'j5_m', 'j5_DL1r', 'j6_pt', 'j6_eta', 'j6_phi', 'j6_m', 'j6_DL1r', 'lep_pt', 'lep_eta', 'lep_phi', 'met_met', 'met_phi']\n",
    "Y_keys = ['th_pt', 'th_eta','th_phi', 'tl_pt', 'tl_eta', 'tl_phi', 'wl_pt', 'wl_eta', 'wl_phi']\n",
    "phi_keys = ['j1_phi', 'j2_phi', 'j3_phi','j4_phi','j5_phi','j6_phi', 'lep_phi', 'met_phi', 'th_phi', 'tl_phi', 'wl_phi']\n",
    "eta_keys = ['j1_eta', 'j2_eta', 'j3_eta', 'j4_eta', 'j5_eta', 'j5_phi', 'j6_eta', 'lep_eta', 'th_eta', 'tl_eta', 'wl_eta']\n",
    "pt_keys = ['j1_pt', 'j2_pt','j3_pt','j4_pt','j5_pt','j6_pt','lep_pt','th_pt', 'tl_pt', 'wl_pt']\n",
    "m_keys = ['j1_m','j2_m', 'j3_m', 'j4_m', 'j5_m', 'j6_m', 'wl_m']\n",
    "DL1r_keys = ['j1_DL1r','j2_DL1r','j3_DL1r','j4_DL1r','j5_DL1r','j6_DL1r']\n",
    "\n",
    "\n",
    "Y_length = len(Y_keys)\n",
    "X_length = len(X_keys)\n",
    "crop0 =  500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max scaling error: 9.663381206337363e-13\n"
     ]
    }
   ],
   "source": [
    "import scale_variables\n",
    "import shape_timesteps\n",
    "\n",
    "# How to scale each input/output\n",
    "def get_methods(keys):\n",
    "    types = {'pt':'cart_pt', 'eta':'meanmax', 'phi':'sincos', 'DL1r':'DL1r', 'm':'meanmax', 'met':'meanmax'}\n",
    "    var_types = [key.split('_')[1] for key in keys]\n",
    "    methods = [types[var] for var in var_types]\n",
    "    return methods\n",
    "\n",
    "X_methods = get_methods(X_keys)\n",
    "Y_methods = get_methods(Y_keys)\n",
    "\n",
    "Scaler = scale_variables.Scale_variables()\n",
    "(X_total, X_maxmean), X_names = Scaler.scale_arrays(X_keys, X_methods, True)\n",
    "(Y_total, Y_maxmean), Y_names = Scaler.scale_arrays(Y_keys, Y_methods, True)\n",
    "\n",
    "error = Scaler.test_inverse(Y_keys, Y_methods, True)\n",
    "print('Max scaling error: {}'.format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450000, 6, 6) (450000, 7) (450000, 12)\n",
      "(50000, 6, 6) (50000, 7) (50000, 12)\n"
     ]
    }
   ],
   "source": [
    "# Create X and test array\n",
    "split = int(np.floor(0.9*crop0)) # 80/20 split \n",
    "\n",
    "trainY, testY = Y_total[0:split,:], Y_total[split:,:]\n",
    "\n",
    "timestep_builder = shape_timesteps.Shape_timesteps()\n",
    "totalX_jets, totalX_other = timestep_builder.reshape_X(X_total, X_names, False,True)\n",
    "\n",
    "trainX_jets, testX_jets = totalX_jets[0:split,:,:], totalX_jets[split:,:,:]\n",
    "trainX_other, testX_other = totalX_other[0:split,:], totalX_other[split:,:]\n",
    "\n",
    "# Save some memory \n",
    "del totalX_jets\n",
    "del totalX_other \n",
    "\n",
    "print(trainX_jets.shape, trainX_other.shape, trainY.shape)\n",
    "print(testX_jets.shape, testX_other.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Histograms of total  Y variables \n",
    "show = False\n",
    "\n",
    "other_bins = np.linspace(-1, 1, 40)\n",
    "phi_bins = np.linspace(-0.1, 0.1, 40)\n",
    "pt_bins = np.linspace(-0.3, 1, 40)\n",
    "Y_bins = [phi_bins if 'phi' in name else pt_bins if 'pt' in name else other_bins for name in Y_names]\n",
    "\n",
    "if show:\n",
    "    plt.figure(figsize=(6,6*trainY.shape[1]))\n",
    "    for i in range(0, trainY.shape[1]):\n",
    "        plt.subplot(trainY.shape[1], 1, i+1)\n",
    "        bins = Y_bins[i]\n",
    "        plt.hist(Y_total[:,i], bins, histtype='step')\n",
    "        plt.xlabel(Y_names[i])\n",
    "        plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of X variables (without the mask)\n",
    "show = False\n",
    "\n",
    "if show:\n",
    "    plt.figure(figsize=(6,6*X_total.shape[1]))\n",
    "    for i in range(0, X_total.shape[1]):\n",
    "        plt.subplot(X_total.shape[1], 1, i+1)\n",
    "        plt.hist(X_total[:,i], 40, histtype='step')\n",
    "        plt.xlabel(X_names[i])\n",
    "        plt.ylabel('Frequency')\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 6, 6)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, 6, 6)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 160)          1280        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 6, 160)       106880      masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 160)          25760       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 6, 160)       205440      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 160)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 960)          0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1120)         0           flatten_1[0][0]                  \n",
      "                                                                 flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          286976      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 160)          41120       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 12)           1932        dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 669,388\n",
      "Trainable params: 669,388\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "\n",
    "def build_model():\n",
    "    jet_input = Input(shape=(trainX_jets.shape[1], trainX_jets.shape[2]))\n",
    "    Mask = Masking(-2)(jet_input)\n",
    "    LSTM11 = LSTM(160, return_sequences=True)(Mask)\n",
    "    LSTM12 = LSTM(160, return_sequences=True)(LSTM11)\n",
    "    \n",
    "    flat_jets = Flatten()(LSTM12)\n",
    "    \n",
    "    other_input = Input(shape=(trainX_other.shape[1]))\n",
    "    Dense21 = Dense(160, activation='relu')(other_input)\n",
    "    Dense22 = Dense(160, activation='relu')(Dense21)\n",
    "    flat_other = Flatten()(Dense22)\n",
    "    \n",
    "    concat = concatenate([flat_other, flat_jets])\n",
    "    dense1 = Dense(256, activation='relu')(concat)\n",
    "    dense2 = Dense(160, activation='relu')(dense1)\n",
    "    output = Dense(len(Y_names), activation='linear')(dense2)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[jet_input, other_input], outputs=output)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-5)\n",
    "    model.compile(loss='mse', optimizer= optimizer, metrics=['mse'])\n",
    "    \n",
    "    return model \n",
    "\n",
    "# model = keras.models.load_model('LSTM_cart_pt.keras')\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/512\n",
      "450000/450000 [==============================] - 30s 66us/sample - loss: 0.0191 - mse: 0.0191 - val_loss: 0.0126 - val_mse: 0.0126\n",
      "Epoch 2/512\n",
      "450000/450000 [==============================] - 17s 39us/sample - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 3/512\n",
      "450000/450000 [==============================] - 17s 38us/sample - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 4/512\n",
      "450000/450000 [==============================] - 17s 38us/sample - loss: 0.0080 - mse: 0.0080 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 5/512\n",
      "450000/450000 [==============================] - 17s 38us/sample - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 6/512\n",
      "450000/450000 [==============================] - 17s 38us/sample - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 7/512\n",
      "450000/450000 [==============================] - 17s 38us/sample - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 8/512\n",
      "450000/450000 [==============================] - 17s 38us/sample - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 9/512\n",
      "450000/450000 [==============================] - 17s 38us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 10/512\n",
      "450000/450000 [==============================] - 17s 38us/sample - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 11/512\n",
      "450000/450000 [==============================] - 17s 38us/sample - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 12/512\n",
      "450000/450000 [==============================] - 17s 38us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 13/512\n",
      "450000/450000 [==============================] - 17s 38us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 14/512\n",
      "450000/450000 [==============================] - 17s 38us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 15/512\n",
      "450000/450000 [==============================] - 17s 38us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 16/512\n",
      "450000/450000 [==============================] - 17s 37us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 17/512\n",
      "450000/450000 [==============================] - 17s 37us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 18/512\n",
      "450000/450000 [==============================] - 17s 37us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 19/512\n",
      "450000/450000 [==============================] - 17s 38us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 20/512\n",
      "450000/450000 [==============================] - 17s 38us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 21/512\n",
      " 15872/450000 [>.............................] - ETA: 16s - loss: 0.0071 - mse: 0.0071"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "Epochs= 512\n",
    "\n",
    "# del X_total\n",
    "# del Y_total\n",
    "\n",
    "history = model.fit([trainX_jets, trainX_other], trainY, verbose=1, epochs=Epochs, \n",
    "                   validation_data=([testX_jets, testX_other], testY), shuffle=False, callbacks=[callback],\n",
    "                   batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'], label='training')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mse loss')\n",
    "plt.legend()\n",
    "plt.title('MSE loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_unscaled = model.predict([testX_jets, testX_other])\n",
    "true_unscaled = testY \n",
    "\n",
    "total_predictions = model.predict([np.append(trainX_jets,testX_jets,axis=0), np.append(trainX_other,testX_other,axis=0)])\n",
    "(Y_total, TO_maxmean0), _ = Scaler.scale_arrays(Y_keys, Y_methods, True)\n",
    "\n",
    "predictions_origscale = Scaler.invscale_arrays(Y_keys, total_predictions, _, Y_methods, TO_maxmean0)[split:,:]\n",
    "true_origscale = Scaler.invscale_arrays(Y_keys, Y_total, _, Y_methods, TO_maxmean0)[split:,:]\n",
    "\n",
    "del Y_total\n",
    "del TO_maxmean0\n",
    "del _\n",
    "del total_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training scale plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import analysis\n",
    "import importlib\n",
    "importlib.reload(analysis)\n",
    "display = analysis.Analysis \n",
    "display.display_errors(predictions_unscaled, true_unscaled, Y_names, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.predictions_vs_sample(predictions_unscaled, true_unscaled, Y_names, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display.variable_histogram(predictions_unscaled, true_unscaled, Y_names, False, Y_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.difference_histogram(predictions_unscaled, true_unscaled, Y_names, False, Y_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display.predicted_vs_true(predictions_unscaled, true_unscaled, Y_names, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    " # Original scale plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.display_errors(predictions_origscale, true_origscale, Y_keys, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.predictions_vs_sample(predictions_origscale, true_origscale, Y_keys, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display.variable_histogram(predictions_origscale, true_origscale, Y_keys, True, [None for name in Y_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display.predicted_vs_true(predictions_origscale, true_origscale, Y_keys, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights, biases = model.layers[0].get_weights()\n",
    "\n",
    "\n",
    "print(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.layers[3].get_weights()\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('LSTM_cart_pt.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
