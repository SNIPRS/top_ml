{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle flow network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from tensorflow.keras.layers import Conv1D, Flatten, Dense, Input, concatenate, Masking, LSTM, TimeDistributed, Bidirectional, RepeatVector\n",
    "# from tensorflow.keras import regularizers \n",
    "# from tensorflow.keras import initializers\n",
    "\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.datasets import qg_jets\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "import h5py \n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "dataset = h5py.File('./../../../../../data/hongtao/variables_tt_re2.h5','r')\n",
    "\n",
    "X_keys = ['j1_pt', 'j1_eta', 'j1_phi', 'j1_m', 'j1_DL1r', 'j2_pt', 'j2_eta', 'j2_phi', 'j2_m', 'j2_DL1r', 'j3_pt', 'j3_eta', 'j3_phi', 'j3_m', 'j3_DL1r', 'j4_pt', 'j4_eta', 'j4_phi', 'j4_m', 'j4_DL1r', 'j5_pt', 'j5_eta', 'j5_phi', 'j5_m', 'j5_DL1r', 'j6_pt', 'j6_eta', 'j6_phi', 'j6_m', 'j6_DL1r', 'lep_pt', 'lep_eta', 'lep_phi', 'met_met', 'met_phi']\n",
    "Y_keys = ['th_pt', 'th_eta','th_phi', 'tl_pt', 'tl_eta', 'tl_phi', 'wl_pt', 'wl_eta', 'wl_phi']\n",
    "phi_keys = list(filter(lambda a: 'phi' in a, dataset.keys()))\n",
    "eta_keys = list(filter(lambda a: 'eta' in a, dataset.keys()))\n",
    "pt_keys =  list(filter(lambda a: 'pt' in a, dataset.keys()))\n",
    "m_keys = list(filter(lambda a: 'm' in a, dataset.keys()))\n",
    "DL1r_keys = list(filter(lambda a: 'DL1r' in a, dataset.keys()))\n",
    "\n",
    "\n",
    "Y_length = len(Y_keys)\n",
    "X_length = len(X_keys)\n",
    "crop0 =  200000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max scaling error: 3.410605131648481e-13\n"
     ]
    }
   ],
   "source": [
    "import scale_variables\n",
    "import shape_timesteps\n",
    "\n",
    "# How to scale each input/output\n",
    "def get_methods(keys):\n",
    "    types = {'pt':'cartbox', 'eta':'meanmax', 'phi':'phi_pi', 'DL1r':'meanmax', 'm':'meanmax', 'met':'meanmax'}\n",
    "    var_types = [key.split('_')[1] for key in keys]\n",
    "    methods = [types[var] for var in var_types]\n",
    "    return methods\n",
    "\n",
    "X_methods = get_methods(X_keys)\n",
    "Y_methods = get_methods(Y_keys)\n",
    "\n",
    "Scaler = scale_variables.Scale_variables()\n",
    "(X_total, X_maxmean), X_names = Scaler.scale_arrays(X_keys, X_methods, True)\n",
    "(Y_total, Y_maxmean), Y_names = Scaler.scale_arrays(Y_keys, Y_methods, True)\n",
    "\n",
    "error = Scaler.test_inverse(Y_keys, Y_methods, True)\n",
    "print('Max scaling error: {}'.format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160000, 7, 5) (160000, 9)\n",
      "(40000, 7, 5) (40000, 9)\n"
     ]
    }
   ],
   "source": [
    "# Create X and test array\n",
    "split = int(np.floor(0.8*crop0)) # 80/20 split \n",
    "\n",
    "trainY, testY = Y_total[0:split,:], Y_total[split:,:]\n",
    "\n",
    "timestep_builder = shape_timesteps.Shape_timesteps()\n",
    "totalX_jets, totalX_other = timestep_builder.reshape_X(X_total, X_names, True,True)\n",
    "\n",
    "mask = timestep_builder.create_mask()\n",
    "totalX_jets = mask*totalX_jets - 2*(1-mask)\n",
    "trainX_jets, testX_jets = totalX_jets[0:split,:,:], totalX_jets[split:,:,:]\n",
    "trainX_other, testX_other = totalX_other[0:split,:], totalX_other[split:,:]\n",
    "\n",
    "trainX_other = np.reshape(trainX_other, (trainX_other.shape[0], 1, -1))\n",
    "testX_other = np.reshape(testX_other, (testX_other.shape[0], 1, -1))\n",
    "trainX = np.concatenate([trainX_other, trainX_jets], axis=1)\n",
    "testX = np.concatenate([testX_other, testX_jets], axis=1)\n",
    "# Save some memory \n",
    "del totalX_jets\n",
    "del totalX_other \n",
    "\n",
    "print(trainX.shape, trainY.shape)\n",
    "print(testX.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of total  Y variables \n",
    "show = False\n",
    "\n",
    "other_bins = np.linspace(-1, 1, 40)\n",
    "phi_bins = np.linspace(-0.1, 0.1, 40)\n",
    "pt_bins = np.linspace(-0.3, 1, 40)\n",
    "Y_bins = [phi_bins if 'phi' in name else pt_bins if 'pt' in name else other_bins for name in Y_names]\n",
    "\n",
    "if show:\n",
    "    plt.figure(figsize=(6,6*Y_total.shape[1]))\n",
    "    for i in range(0, Y_total.shape[1]):\n",
    "        plt.subplot(Y_total.shape[1], 1, i+1)\n",
    "        bins = Y_bins[i]\n",
    "        plt.hist(Y_total[:,i], bins, histtype='step')\n",
    "        plt.xlabel(Y_names[i])\n",
    "        plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of X variables (without the mask)\n",
    "show = False\n",
    "\n",
    "if show:\n",
    "    plt.figure(figsize=(6,6*X_total.shape[1]))\n",
    "    for i in range(0, X_total.shape[1]):\n",
    "        plt.subplot(X_total.shape[1], 1, i+1)\n",
    "        plt.hist(X_total[:,i], 40, histtype='step')\n",
    "        plt.xlabel(X_names[i])\n",
    "        plt.ylabel('Frequency')\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 5)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 100)    600         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 100)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 100)    10100       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 100)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 256)    25856       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 256)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 256)          0           mask[0][0]                       \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 256)          65792       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 256)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          32896       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 9)            1161        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 9)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 136,405\n",
      "Trainable params: 136,405\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes, F_sizes = (100, 100, 256), (256, 128)\n",
    "# Phi_sizes, F_sizes = (100, 100, 256), (100, 100, 100)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=5e-5)\n",
    "\n",
    "pfn = PFN(input_dim=trainX.shape[-1], Phi_sizes=Phi_sizes, F_sizes=F_sizes, mask_val=-2, output_dim=trainY.shape[-1], output_act='linear', loss='mse', optimizer=optimizer, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160000 samples, validate on 40000 samples\n",
      "Epoch 1/256\n",
      "160000/160000 [==============================] - 3s 17us/sample - loss: 0.1442 - mse: 0.1442 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 2/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0271 - mse: 0.0271 - val_loss: 0.0230 - val_mse: 0.0230\n",
      "Epoch 3/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0207 - mse: 0.0207 - val_loss: 0.0192 - val_mse: 0.0192\n",
      "Epoch 4/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0181 - mse: 0.0181 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 5/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0161 - val_mse: 0.0161\n",
      "Epoch 6/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0152 - val_mse: 0.0152\n",
      "Epoch 7/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 8/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 9/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 10/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 11/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0130 - val_mse: 0.0130\n",
      "Epoch 12/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 13/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0126 - val_mse: 0.0126\n",
      "Epoch 14/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0123 - val_mse: 0.0123\n",
      "Epoch 15/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 16/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0120 - val_mse: 0.0120\n",
      "Epoch 17/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 18/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 19/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 20/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 21/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 22/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 23/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 24/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 25/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0109 - val_mse: 0.0109\n",
      "Epoch 26/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0107 - val_mse: 0.0107\n",
      "Epoch 27/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 28/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 29/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 30/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 31/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 32/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 33/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 34/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 35/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 36/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 37/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 38/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 39/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 40/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 41/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 42/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 43/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 44/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 45/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 46/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 47/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 48/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 49/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 50/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 51/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 52/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 53/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 54/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 55/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 56/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 57/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0091 - val_mse: 0.0091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 59/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 60/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 61/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 62/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 63/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 64/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 65/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 66/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 67/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 68/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 69/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 70/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 71/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 72/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 73/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 74/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 75/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 76/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 77/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 78/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 79/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 80/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 81/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 82/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 83/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 84/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 85/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 86/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 87/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 88/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 89/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 90/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 91/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 92/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 93/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 94/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0080 - mse: 0.0080 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 95/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0080 - mse: 0.0080 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 96/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0080 - mse: 0.0080 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 97/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0080 - mse: 0.0080 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 98/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0080 - mse: 0.0080 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 99/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0080 - mse: 0.0080 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 100/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0080 - mse: 0.0080 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 101/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 102/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 103/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 104/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 105/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 106/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 107/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 108/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 109/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 110/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 111/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 112/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 113/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 114/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 115/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 116/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 117/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 118/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 119/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 120/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 121/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 122/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 123/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 124/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 125/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 126/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 127/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 128/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 129/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 130/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 131/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 132/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 133/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 134/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 135/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 136/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 137/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 138/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 139/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 140/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 141/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 142/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 143/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 144/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 145/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 146/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 147/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 148/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 149/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 150/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 151/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 152/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 153/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 154/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 155/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 156/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 157/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 158/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 159/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 160/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 161/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 162/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 163/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 164/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 165/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 166/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 167/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 168/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 169/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 170/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 171/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0075 - val_mse: 0.0075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 173/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 174/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 175/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 176/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 177/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 178/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 179/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 180/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 181/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 182/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 183/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 184/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 185/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 186/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 187/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 188/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 189/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 190/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 191/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 192/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 193/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 194/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 195/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 196/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 197/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 198/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 199/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 200/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 201/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 202/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 203/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 204/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 205/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 206/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 207/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 208/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 209/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 210/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 211/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 212/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 213/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 214/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 215/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 216/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 217/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 218/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 219/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 220/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 221/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 222/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 223/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 224/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 225/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 226/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 227/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 228/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 230/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 231/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 232/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 233/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 234/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 235/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 236/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 237/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 238/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 239/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 240/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 241/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 242/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 243/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 244/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 245/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 246/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 247/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 248/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 249/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 250/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 251/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 252/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 253/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 254/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 255/256\n",
      "160000/160000 [==============================] - 2s 11us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 256/256\n",
      "160000/160000 [==============================] - 2s 10us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0072 - val_mse: 0.0072\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "# network training parameters\n",
    "num_epoch = 256\n",
    "batch_size = 500\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history = pfn.fit(trainX, trainY,\n",
    "          epochs=num_epoch,\n",
    "          batch_size=batch_size, \n",
    "                  callbacks=[callback],\n",
    "          validation_data=(testX, testY),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'], label='training')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mse loss')\n",
    "plt.legend()\n",
    "plt.title('MSE loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analysis:\n",
    "    def __init__(self):\n",
    "        self.crop = 100\n",
    "        self.fsize = 6\n",
    "        self.histogram_bins = 40\n",
    "        \n",
    "    def wrap_phi(self, var):\n",
    "        var = var%(2*np.pi)\n",
    "        var = var - 2*np.pi*(var > np.pi)\n",
    "        return var\n",
    "        \n",
    "    def predictions_vs_sample(self, compare, true, names, wrap_phi):\n",
    "        plt.figure(figsize=(self.fsize*2, self.fsize*len(names)))\n",
    "        for i in range(0, len(names)):\n",
    "            compare_small = compare[:self.crop,i]\n",
    "            true_small = true[:self.crop,i]\n",
    "            if wrap_phi and \"phi\" in names[i]:\n",
    "                compare_small = self.wrap_phi(compare_small)\n",
    "                true_small = self.wrap_phi(true_small)\n",
    "            plt.subplot(len(names), 1, i+1)\n",
    "            plt.plot(range(0,self.crop), compare_small, 'bo', markersize=3, label = 'Predictions')\n",
    "            plt.plot(range(0,self.crop), true_small, 'ro', markersize=3, label = 'True Value')\n",
    "            ym, yM = plt.ylim()\n",
    "            for x in range(self.crop):\n",
    "                plt.vlines(x, color='g', linestyle='-', alpha=0.2, ymin= \n",
    "                            min(compare_small[x], true_small[x]), \n",
    "                            ymax= max(compare_small[x], true_small[x]))\n",
    "            plt.hlines(np.mean(true[:,i]), xmin=-20, xmax=self.crop+20, alpha=0.5)\n",
    "            MSE = 1/compare[:,i].size*np.sum((compare[:,i]- true[:,i])**2)\n",
    "            plt.xlabel('Sample')\n",
    "            plt.xlim(0, self.crop)\n",
    "            plt.ylabel(names[i])\n",
    "            plt.title(names[i] + \" MSE: \" + str(MSE))\n",
    "            plt.legend()\n",
    "    \n",
    "    def display_errors(self, compare, true, names, wrap_phi):\n",
    "        MSE = 1/compare.size*np.sum((compare- true)**2)\n",
    "        print(\"total MSE: \" + str(MSE))\n",
    "        print(\" \")\n",
    "        for i in range(len(names)):\n",
    "            diff = compare[:,i] -true[:,i]\n",
    "            if wrap_phi and \"phi\" in names[i]:\n",
    "                diff = self.wrap_phi(diff)\n",
    "            MSE = 1/compare[:,i].size*np.sum((diff)**2)\n",
    "            print(\"{0} MSE : \".format(names[i]), '%.10f'%MSE)\n",
    "    \n",
    "    def difference_histogram(self, compare, true, names, wrap_phi, bins):\n",
    "        plt.figure(figsize=(self.fsize*2,self.fsize*len(names)))\n",
    "        for i in range(len(names)):\n",
    "            plt.subplot(len(names), 1, i+1)\n",
    "            diff = true[:,i] - compare[:,i]\n",
    "            hist0, bin_edges = np.histogram(true[:, i], bins=40)\n",
    "            if bins[i] is None:\n",
    "                hbins = bin_edges\n",
    "            else:\n",
    "                hbins = bins[i]\n",
    "            plt.hist(diff, hbins, histtype='step', color='purple', label='true - predicted', density=True)\n",
    "            plt.xlabel(\"Difference (Mean: {0}, Std: {1})\".format(np.mean(diff), np.std(diff)))\n",
    "            plt.title(names[i])\n",
    "            plt.legend()\n",
    "            plt.ylabel('Frequency')\n",
    "            \n",
    "    def variable_histogram(self, compare, true, names, wrap_phi, bins): \n",
    "        plt.figure(figsize=(self.fsize*2,self.fsize*len(names)))\n",
    "        for i in range(len(names)):\n",
    "            plt.subplot(len(names), 1, i+1)\n",
    "            compare_small = compare[:, i]\n",
    "            true_small = true[:, i]\n",
    "            if wrap_phi and \"phi\" in names[i]:\n",
    "                compare_small = self.wrap_phi(compare_small)\n",
    "                true_small = self.wrap_phi(true_small)\n",
    "            hist0, bin_edges = np.histogram(true[:, i], bins=40)\n",
    "            \n",
    "            if bins[i] is None:\n",
    "                hbins = bin_edges\n",
    "            else:\n",
    "                hbins = bins[i]\n",
    "                \n",
    "            plt.hist(true_small, hbins, histtype='step', color='b', label='true values', density=False)\n",
    "            plt.hist(compare_small, hbins, histtype='step', color='r', label='predictions', density=False)\n",
    "            plt.xlabel(names[i])\n",
    "            plt.title(names[i])\n",
    "            plt.legend()\n",
    "            plt.ylabel('Frequency')\n",
    "    \n",
    "    def difference_vs_variable(self, compare, true, names, wrap_phi):\n",
    "        plt.figure(figsize=(self.fsize*2,self.fsize*len(names)))\n",
    "        for i in range(len(names)):\n",
    "            plt.subplot(len(names), 1, i+1)\n",
    "            plt.plot(true[:, i], true[:, i]-compare[:, i], 'o', color='purple', label='True - Predicted', markersize=2)\n",
    "            plt.xlabel('True ' + names[i])\n",
    "            plt.legend()\n",
    "            plt.ylabel('Difference')\n",
    "    \n",
    "    def predicted_vs_true(self, compare, true, names, wrap_phi):\n",
    "        plt.figure(figsize=(self.fsize*2,self.fsize*len(names)))\n",
    "        for i in range(len(names)):\n",
    "            plt.subplot(len(names), 1, i+1)\n",
    "            plt.plot(true[:, i], compare[:, i], 'o', color='g', markersize=2)\n",
    "            line = np.linspace(np.min(true[:, i]), np.max(true[:, i]), 100)\n",
    "            plt.plot(line, line, color='b')\n",
    "            plt.xlabel('True')\n",
    "            plt.title(names[i])\n",
    "            plt.ylabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Analysis()\n",
    "\n",
    "predictions_unscaled = pfn.predict(testX)\n",
    "true_unscaled = testY \n",
    "\n",
    "total_predictions = pfn.predict(np.append(trainX,testX,axis=0))\n",
    "(Y_total, TO_maxmean0), _ = Scaler.scale_arrays(Y_keys, Y_methods, True)\n",
    "\n",
    "predictions_origscale = Scaler.invscale_arrays(Y_keys, total_predictions, _, Y_methods, TO_maxmean0)[split:,:]\n",
    "true_origscale = Scaler.invscale_arrays(Y_keys, Y_total, _, Y_methods, TO_maxmean0)[split:,:]\n",
    "\n",
    "del Y_total\n",
    "del TO_maxmean0\n",
    "del _\n",
    "del total_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training scale plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display.display_errors(predictions_unscaled, true_unscaled, Y_names, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.predictions_vs_sample(predictions_unscaled, true_unscaled, Y_names, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display.variable_histogram(predictions_unscaled, true_unscaled, Y_names, False, Y_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.difference_histogram(predictions_unscaled, true_unscaled, Y_names, False, Y_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display.predicted_vs_true(predictions_unscaled, true_unscaled, Y_names, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    " # Original scale plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.display_errors(predictions_origscale, true_origscale, Y_keys, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.predictions_vs_sample(predictions_origscale, true_origscale, Y_keys, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display.variable_histogram(predictions_origscale, true_origscale, Y_keys, True, [None for name in Y_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display.predicted_vs_true(predictions_origscale, true_origscale, Y_keys, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights, biases = model.layers[0].get_weights()\n",
    "\n",
    "\n",
    "print(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.layers[3].get_weights()\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
